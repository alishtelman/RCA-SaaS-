# llm/generator.py
import os
import requests

# URL и модель Ollama
OLLAMA_URL = os.getenv("LLM_REMOTE_URL", "http://host.docker.internal:11434")
LLM_MODEL = os.getenv("LLM_MODEL", "qwen2.5:7b-instruct-q4_K_M")  # или твоя модель


def build_prompt(context: str, question: str) -> str:
    """
    Строим один большой prompt для модели.
    Важно: явно просим СТРОГУЮ структуру 1–4 и запрещаем копировать контекст/инструкции.
    """
    return f"""Ты — инженер технической поддержки банка, который помогает разбирать обращения клиентов
по мобильному и веб-банку. Твоя задача — по новому инциденту и историям прошлых заявок
сформулировать понятный и практичный план действий для инженера 1-й линии.

Правила:
- Отвечай ТОЛЬКО по-русски.
- Не пиши «воды» и общих фраз, используй конкретику.
- НЕ копируй текст контекста и самого запроса дословно.
- НЕ повторяй формулировки инструкций (типа «1–3 пункта, что именно не работает у клиента»),
  вместо них обязательно подставь реальные содержательные пункты.
- Строго соблюдай структуру из ЧЕТЫРЁХ разделов (1–4), никаких других заголовков и хвостов.

Ниже дан контекст — фрагменты прошлых инцидентов. В нём есть шум, используй только то,
что реально помогает понять проблему и способы её решения.

КОНТЕКСТ:
{context}

НОВЫЙ ИНЦИДЕНТ:
\"\"\"{question}\"\"\"

Сначала проанализируй новый инцидент и сопоставь его с контекстом.
Затем дай ответ СТРОГО в ЧЕТЫРЁХ разделах, в таком формате (замени подсказки на реальные тексты):

1) Описание проблемы:
- 1–3 пункта, что именно не работает у клиента (коротко, по делу: что делал, что ожидал, что получил).

2) Возможные причины:
- 3–5 конкретных гипотез по категориям: клиент/устройство/сеть/аккаунт/версия/сервер/интеграции и т.п.

3) Рекомендуемые действия:
- 3–6 чётких пошаговых действий для оператора 1-й линии:
  проверки в системах банка, действия в мобильном/веб-банке, проверки ограничений/блокировок,
  что спросить у клиента, какие настройки/операции попробовать.

4) Следующие шаги/эскалация:
- Что собрать и куда эскалировать, если шаги выше не помогли:
  какие данные запросить (ИИН/телефон/OS/App версия/тип устройства/тип сети/скрины/точное время/ошибка),
  в какую группу/подразделение передать, с какой формулировкой.

Важно:
- Не добавляй никакого текста ДО раздела 1) и ПОСЛЕ раздела 4).
- Не вставляй сюда снова «КОНТЕКСТ», «НОВЫЙ ИНЦИДЕНТ» и другие части промпта.
"""


def generate(context: str, question: str) -> str:
    """
    Вызывает Ollama и возвращает сырой текст ответа от модели.
    """
    prompt = build_prompt(context, question)

    payload = {
        "model": LLM_MODEL,
        "prompt": prompt,
        "stream": False,
        # Можно слегка ограничить длину, чтобы модель не раздувалась
        "options": {
            "num_predict": 768
        },
    }

    resp = requests.post(
        f"{OLLAMA_URL}/api/generate",
        json=payload,
        timeout=120,
    )
    resp.raise_for_status()
    data = resp.json()
    return (data.get("response") or "").strip()
